# -*- coding: utf-8 -*-
"""Scraper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14PMKzwSrMMRXFYBN8uj9YTYjUm8HKnKu
"""

# Import necessary modules
import urllib.request  # Module for handling URLs

from bs4 import BeautifulSoup  # BeautifulSoup for HTML parsing

import os  # Module for interacting with the operating system

# Define a class named Scraper
class Scraper:

    # Constructor method to initialize the object with a website URL
    def __init__(self, site):

        self.site = site  # Assign the provided website URL

        # Default output file name
        self.output_file = "output.txt"

    # Method to perform web scraping
    def scrape(self):

        # Open a connection to the website
        r = urllib.request.urlopen(self.site)

        # Read the HTML content of the page
        html = r.read()

        # Specify the HTML parser
        parser = "html.parser"

        # Create a BeautifulSoup object to parse the HTML
        sp = BeautifulSoup(html, parser)

        # Check if the output file already exists
        if os.path.exists(self.output_file):

            # If it exists, find the next available file name
            index = 1
            while os.path.exists(f"output({index}).txt"):
                index += 1

            # Update the output file name
            self.output_file = f"output({index}).txt"

        # Open the output file in write mode
        with open(self.output_file, "w") as file:

            # Loop through all <a> tags in the HTML
            for tag in sp.find_all("a"):

                # Get the "href" attribute of the <a> tag
                url = tag.get("href")

                # Skip if the URL is None
                if url is None:
                    continue

                # Write the URL to the output file if it contains "articles"
                if "articles" in url:
                    file.write(url + "\n")

# Example usage
# Define the website URL
news = "https://news.google.com/"

# Create a Scraper object and perform scraping
Scraper(news).scrape()